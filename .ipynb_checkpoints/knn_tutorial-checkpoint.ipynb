{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (K-nearest neighbors) e uma visão inicial sobre problemas de classificação \n",
    "\n",
    "<small>Por: Hugo Soares, Cefas Rodrigues e Arilton Filho</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visão Geral \n",
    "\n",
    "O KNN (K-nearest neighbors, <i> K-vizinhos mais próximos</i>) é um dos mais famosos e simples algoritmos de Machine Learning (ML). Ele é o pontapé inicial da maioria dos entusiastas dessa área, pois além de possuir uma implementação simples, também é bastante intuitivo. Quando utilizado para problemas de classificação, ele tem como objetivo classificar uma amostra de classe desconhecida com base na comparação com outros exemplos rotulados (já classificados). Ou seja, ele implementa um aprendizado baseado em instâncias, <i>Instanced Based Learning</i>. \n",
    "    <img src=\"img/Instance-based-Algorithms.png\"> \n",
    "    <small>Fonte: https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ele compara uma amostra não-rotulada com exemplos classificados (já rotulados), dizemos que ele implementa uma aprendizagem supervisionada. Ou seja, o aprendizado do algoritmo se baseia na supervisão de especialistas que anteriormente classificoram os exemplos que servirão de comparação. Assim, cada registro do meu conjunto de dados possui n atributos e uma classe que o rotula. Já a minha amostra possui apenas os atributos. O conjunto de exemplos que utilizamos para realizar as comparações é chamado de Conjunto de Treino (<i>Training Set</i>), logo cada exemplo é chamado de Instância de Treino (<i>Training Instance</i>) \n",
    "\n",
    "<img src=\"img/supervisioned_learning.png\">\n",
    "<small>Fonte: http://bigdata-madesimple.com/machine-learning-explained-understanding-supervised-unsupervised-and-reinforcement-learning/ </small>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset do estudo de caso\n",
    "\n",
    "Por exemplo, em nosso estudo de caso utilizaremos o Haberman's Survival Data Set. Ele é composto por 306 registros de um estudo realizado sobre a sobrevida de pacientes submetidos à cirurgia para câncer de mama. Cada registro possui 3 atributos (idade do paciente na época da operação, ano da operação e o número de nódulos axilares positivos detectados) e uma classe que pode ser 1 ou 2 (1 significa que o paciente sobreviveu por 5 anos ou mais e 2 significa que o paciente morreu dentro de 5 anos). \n",
    "\n",
    "<img src=\"img/haberman.png\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação entre a amostra e os exemplos\n",
    "\n",
    "Já sabemos que o KNN se baseia na comparação de uma amostra de entrada com exemplos já rotulados. Mas como ocorre esse comparação? É nesse ponto que reside a simplicidade do algoritmo. Ele somente compara amostra e um exemplo através da <b>distância euclideana</b> entre eles... \n",
    "<br>\n",
    "<p style=\"font-size:13px\">Relembrando, distância euclidiana é a distância entre dois pontos que é definida através do teorema de pitágoras. A distância euclidiana entre a amostra <i>X</i> e o exemplo <i>Y</i> é mostrada abaixo. Onde X1, X2, ..., Xn são os atributos de X e Y1, Y2, ..., Yn são os atributos Y.</p> \n",
    "\n",
    "\\begin{align}\n",
    "d(x, y) = \\sqrt{ (x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2 }\n",
    "\\end{align}\n",
    "\n",
    "Em seguida ele verifica quais são os K-vizinhos mais próximos da amostra (exemplos com as menores distâncias euclidianas). De acordo com as classes desse vizinhos podemos classificar a amostra. Sendo K=1, verificaremos apenas qual a classe do exemplo mais próximo da entrada e  a indicamos como a possível classe da amostra. Sendo k=3 verificaremos qual a classe que mais se repete nesses 3 vizinhos e a idicamos como classe da amostra. Além disso, podemos indicar essa classe em termos de probabiblidade. Por exemplo, para k=10 se 6 dos vizinhos mais próximos apresentarem a classe A, podemos informar que a amostra tem 60% de chances de pertencer a classe A.  \n",
    "\n",
    "<img src=\"img/knn_ilustracao.png\"> \n",
    "<small>Fonte: https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E como podemos analisar a acurácia do nosso algoritmo? Primeiramente dividimos nosso conjunto total de registros rotulados em dois subconjuntos: o conjunto de treino (que já foi explicado) e o conjunto de treino. Em geral, selecionamos randomicamente 70% dos registros para treino e 30% para teste. \n",
    "<br><br>\n",
    "Em seguida, nós aplicamos cada um dos registros de teste ao algoritmo de classificação que construímos e verificamos se a classe gerada pelo algoritmo corresponde a verdadeira classe da instância de teste. Assim, se tivermos 200 exemplos no conjunto de teste e o nosso algoritmo acertar a classe de 190 deles, podemos dizer que sua acurácia é de 95%!  \n",
    "\n",
    "<img src=\"img/acuracia.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos Implementar! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
