{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (K-nearest neighbors) e uma visão inicial sobre problemas de classificação \n",
    "\n",
    "<small>Por: Hugo Soares, Cefas Rodrigues e Arilton Filho</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visão Geral \n",
    "\n",
    "O KNN (K-nearest neighbors, <i> K-vizinhos mais próximos</i>) é um dos mais famosos e simples algoritmos de Machine Learning (ML). Ele é o pontapé inicial da maioria dos entusiastas dessa área, pois além de possuir uma implementação simples, também é bastante intuitivo. Quando utilizado para problemas de classificação, ele tem como objetivo classificar uma amostra de classe desconhecida com base na comparação com outros exemplos rotulados (já classificados). Ou seja, ele implementa um aprendizado baseado em instâncias, <i>Instanced Based Learning</i>. \n",
    "    <img src=\"img/Instance-based-Algorithms.png\"> \n",
    "    <small>Fonte: https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ele compara uma amostra não-rotulada com exemplos classificados (já rotulados), dizemos que ele implementa uma aprendizagem supervisionada. Ou seja, o aprendizado do algoritmo se baseia na supervisão de especialistas que anteriormente classificoram os exemplos que servirão de comparação. Assim, cada registro do meu conjunto de dados possui n atributos e uma classe que o rotula. Já a minha amostra possui apenas os atributos. O conjunto de exemplos que utilizamos para realizar as comparações é chamado de Conjunto de Treino (<i>Training Set</i>), logo cada exemplo é chamado de Instância de Treino (<i>Training Instance</i>) \n",
    "\n",
    "<img src=\"img/supervisioned_learning.png\">\n",
    "<small>Fonte: http://bigdata-madesimple.com/machine-learning-explained-understanding-supervised-unsupervised-and-reinforcement-learning/ </small>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset do estudo de caso\n",
    "\n",
    "Por exemplo, em nosso estudo de caso utilizaremos o Haberman's Survival Data Set. Ele é composto por 306 registros de um estudo realizado sobre a sobrevida de pacientes submetidos à cirurgia para câncer de mama. Cada registro possui 3 atributos (idade do paciente na época da operação, ano da operação e o número de nódulos axilares positivos detectados) e uma classe que pode ser 1 ou 2 (1 significa que o paciente sobreviveu por 5 anos ou mais e 2 significa que o paciente morreu dentro de 5 anos). \n",
    "\n",
    "<img src=\"img/haberman.png\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação entre a amostra e as intâncias do conjunto de treino\n",
    "\n",
    "Já sabemos que o KNN se baseia na comparação de uma amostra de entrada com exemplos já rotulados. Mas como ocorre esse comparação? É nesse ponto que reside a simplicidade do algoritmo. Ele somente compara amostra e um exemplo através da <b>distância euclideana</b> entre eles... \n",
    "<br>\n",
    "<p style=\"font-size:13px\">Relembrando, distância euclidiana é a distância entre dois pontos que é definida através do teorema de pitágoras. A distância euclidiana entre a amostra <i>X</i> e o exemplo <i>Y</i> é mostrada abaixo. Onde X1, X2, ..., Xn são os atributos de X e Y1, Y2, ..., Yn são os atributos Y.</p> \n",
    "\n",
    "\\begin{align}\n",
    "d(x, y) = \\sqrt{ (x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2 }\n",
    "\\end{align}\n",
    "\n",
    "Em seguida ele verifica quais são os K-vizinhos mais próximos da amostra (exemplos com as menores distâncias euclidianas). De acordo com as classes desse vizinhos podemos classificar a amostra. Sendo K=1, verificaremos apenas qual a classe do exemplo mais próximo da entrada e  a indicamos como a possível classe da amostra. Sendo k=3 verificaremos qual a classe que mais se repete nesses 3 vizinhos e a idicamos como classe da amostra. Além disso, podemos indicar essa classe em termos de probabiblidade. Por exemplo, para k=10 se 6 dos vizinhos mais próximos apresentarem a classe A, podemos informar que a amostra tem 60% de chances de pertencer a classe A.  \n",
    "\n",
    "<img src=\"img/knn_ilustracao.png\"> \n",
    "<small>Fonte: https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E como podemos analisar a acurácia do nosso algoritmo? Primeiramente dividimos nosso conjunto total de registros rotulados em dois subconjuntos: o conjunto de treino (que já foi explicado) e o conjunto de treino. Em geral, selecionamos randomicamente 70% dos registros para treino e 30% para teste. \n",
    "<br><br>\n",
    "Em seguida, nós aplicamos cada um dos registros de teste ao algoritmo de classificação que construímos e verificamos se a classe gerada pelo algoritmo corresponde a verdadeira classe da instância de teste. Assim, se tivermos 200 exemplos no conjunto de teste e o nosso algoritmo acertar a classe de 190 deles, podemos dizer que sua acurácia é de 95%!  \n",
    "\n",
    "<img src=\"img/acuracia.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos Implementar! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando nossos dados\n",
    "\n",
    "Primeiro, importamos nosso dataset de um arquivo externo. Em seguida, o normalizamos, ou seja, deixamos todos os ATRIBUTOS entre 0-1. a coluna da classe permanece igual. Fazemos isso para equilibrar a contribuição de cada atributo no cálculo da distância. Imagine o caso em que o atributo A está no intervalo 0-5 e o atributo B em 0-30000. É fácil perceber que se NÃO houver normalização, possivelmente variações no atributo B terão um impacto muito maior do que variações no atributo A no cálculo da distância entre a amostra e um exemplo qualquer. \n",
    "\n",
    "Em seguida, dividimos nosso dataset normalizado em dois subconjuntos: o conjunto de treino e o de teste. Nesse caso, nosso conjunto de treino possui 2/3 do dataset e o restante pertence ao conjunto de treino. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def processar_dataset(nome_arquivo: str):\n",
    "    \"\"\"\n",
    "    Importa os dados de um arquivo e divide o dataset \n",
    "    em um conjunto de treino e outro de teste\n",
    "    :param nome_arquivo: nome do arquivo externo\n",
    "    \"\"\"\n",
    "    dataset = np.loadtxt(nome_arquivo, delimiter=',')\n",
    "    dataset[:, :3] = normalize(dataset[:,:3]) #Normalizacao apenas dos atributos\n",
    "\n",
    "    n = dataset.shape[0] #Quantidade de registros do nosso dataset\n",
    "    np.random.shuffle(dataset)  #Embaralha o dataset \n",
    "    conjunto_de_treino = dataset[(n//3):, :] #recebe os elementos com indices > n/3\n",
    "    conjunto_de_teste = dataset[:(n//3), :] #recebe os elementos com indices < n/3\n",
    "\n",
    "    print(\"Dimensão do conjunto de treino:\", conjunto_de_treino.shape)\n",
    "    print(\"DImensão do conjunto de teste:\", conjunto_de_teste.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos encontrar os vizinhos? \n",
    "\n",
    "Agora, criamos uma função que recebe uma amostra e procura no conjunto de treino os k-vizinhos mais próximos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([70., 58.,  0.,  2.]), 2.0),\n",
       " (array([73., 62.,  0.,  1.]), 4.123105625617661),\n",
       " (array([70., 58.,  4.,  2.]), 4.47213595499958),\n",
       " (array([75., 62.,  1.,  1.]), 5.0990195135927845),\n",
       " (array([67., 61.,  0.,  1.]), 5.830951894845301)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "import operator\n",
    "\n",
    "def encontrar_vizinhos(amostra: object, k : int=1):\n",
    "    \"\"\"\n",
    "    Recebe uma amostra e encontra os k-vizinhos mais próximos do conjunto\n",
    "    treino. \n",
    "    \n",
    "    :param amostra: amostra que deve ser comparada com as instancias\n",
    "        de treino\n",
    "    :param k: quantidade de vizinhos selecionados\n",
    "    :return: array com os k-vizinhos e suas distancias  \n",
    "    \"\"\"\n",
    "    vizinhos_com_distancias = [ [[], 0] ] * conjunto_de_treino.shape[0]\n",
    "    \n",
    "    for (index, vizinho) in enumerate(conjunto_de_treino): \n",
    "        distancia = distance.euclidean(amostra, vizinho[:3])\n",
    "        vizinhos_com_distancias[index] = (vizinho, distancia)\n",
    "        \n",
    "    vizinhos_com_distancias.sort(key=operator.itemgetter(1))\n",
    "    \n",
    "    return vizinhos_com_distancias[:k]\n",
    "\n",
    "encontrar_vizinhos(conjunto_de_teste[4, :3], 5) # testando com o 4 elemento do conjunto de teste. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolhendo a classe \n",
    "\n",
    "Com base nos vizinhos mais próximos, podemos escolher a classe com a qual rotularemos amostra. O jeito mais simples de fazer isso, é vendo qual a classe que mais se repete nos k-vizinhos mais próximos. Outra forma de fazer isso, seria similar a uma votação com pesos, onde o peso do \"voto\" de cada vizinho seria inversamente proporcional a distância entre ele e a amostra. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def escolher_classe(k_vizinhos: object):\n",
    "    \"\"\"\n",
    "    :param kvizinhos: os k vizinhos e suas distancias\n",
    "    :return: retorna com maior ocorrência nos vizinhos\n",
    "    \"\"\"\n",
    "    \n",
    "    respostas = {}\n",
    "    for vizinho in k_vizinhos:\n",
    "        resposta = vizinho[0][-1]\n",
    "        if resposta in respostas:\n",
    "            respostas[resposta] += 1 / (1+vizinho[1])\n",
    "        else:\n",
    "            respostas[resposta] = 1 / (1 + vizinho[1])\n",
    "            \n",
    "    respostas = OrderedDict(sorted(respostas.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    return list(respostas.items())[0][0]\n",
    "\n",
    "z = [ ([70., 58.,  0.,  2.], 2.0), ([70., 58.,  0.,  2.], 2.0), \n",
    "     ([73., 62.,  0.,  1.], 4.123105625617661),\n",
    "     ([70., 58.,  4.,  2.], 4.47213595499958) ]\n",
    "\n",
    "escolher_classe(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntando tudo... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main():\n",
    "    processar_dataset('data/haberman.data')\n",
    "    respostas_certas = 0 \n",
    "    \n",
    "    for amostra in conjunto_de_teste: \n",
    "        k_vizinhos = encontrar_vizinhos(amostra[:3], 15)\n",
    "        classe = escolher_classe(k_vizinhos)\n",
    "        if amostra[3] == classe: respostas_certas += 1\n",
    "    \n",
    "    \n",
    "    precisao = (respostas_certas/conjunto_de_teste.shape[0])*100\n",
    "    \n",
    "    plt.figure(figsize=(200,200))\n",
    "    vor = Voronoi(conjunto_de_treino[:, 0:2])\n",
    "    plt.voronoi_plot_2d(vor, show_vertices=0)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Precisão do algoritmo: \", precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão do conjunto de treino: (204, 4)\n",
      "DImensão do conjunto de teste: (102, 4)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'voronoi_plot_2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-312-c760a91efcff>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mvor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVoronoi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconjunto_de_treino\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoronoi_plot_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_vertices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'voronoi_plot_2d'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 14400x14400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
